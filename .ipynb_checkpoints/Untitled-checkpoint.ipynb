{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinwei/anaconda/envs/tensorflow1.40/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Current Path: /Users/jinwei/Documents/GitHub/morph_points\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import socket\n",
    "import os\n",
    "import sys\n",
    "import provider_zj\n",
    "BASE_DIR = os.getcwd()\n",
    "print('The Current Path:', BASE_DIR)\n",
    "import h5py\n",
    "sys.path.append(os.path.join(BASE_DIR, 'model'))\n",
    "sys.path.append(os.path.join(BASE_DIR,'utils'))\n",
    "import transform_nets\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# para set\n",
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 1024 \n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.7\n",
    "LOG_DIR = 'log'\n",
    "#\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mkdir log file\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'),'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learing_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "    return learning_rate \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_global_feature_net(points,is_training_pl,bn_decay):\n",
    "    batch_size = points.get_shape()[0].value\n",
    "    num_points = points.get_shape()[1].value\n",
    "    end_points = {}\n",
    "    with tf.variable_scope('transform_net1') as sc:\n",
    "        transform = transform_nets.input_transform_net(points, is_training_pl,bn_decay,K=3)\n",
    "    points_transformed = tf.matmul(points, transform)\n",
    "    input_image = tf.expand_dims(points_transformed,-1)\n",
    "    \n",
    "    net = tf_util.conv2d(input_image, 64, [1,3], scope='scope',\n",
    "                        stride=[1,1], padding='VALID', bn=False, \n",
    "                        bn_decay=bn_decay,is_training=is_training_pl)\n",
    "    net = tf_util.conv2d(net, 64, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=False, is_training=is_training_pl,\n",
    "                        scope='conv2', bn_decay=bn_decay)\n",
    "    with tf.variable_scope('transform_net2') as sc:\n",
    "        transform = transform_nets.feature_transform_net(net, is_training_pl, bn_decay, K=64)\n",
    "    end_points['transform'] = transform\n",
    "    net_transformed = tf.matmul(tf.squeeze(net), transform)\n",
    "    net_transformed = tf.expand_dims(net_transformed,[2])\n",
    "    \n",
    "    points_feature = net_transformed\n",
    "    \n",
    "    net = tf_util.conv2d(net_transformed, 64, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=False, is_training=is_training_pl,\n",
    "                        scope='conv3', bn_decay=bn_decay)\n",
    "    \n",
    "    net = tf_util.conv2d(net_transformed, 128, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=False, is_training=is_training_pl,\n",
    "                        scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net_transformed, 1024, [1,1], padding='VALID',\n",
    "                        stride=[1,1], bn=False, is_training=is_training_pl,\n",
    "                        scope='conv5', bn_decay=bn_decay)\n",
    "    net = tf_util.max_pool2d(net,[num_points,1], padding='VALID', \n",
    "                            scope='maxpooling')\n",
    "    print(net)\n",
    "    net = tf.reshape(net, [batch_size, -1])\n",
    "    \n",
    "    return net, points_feature, end_points\n",
    "\n",
    "\n",
    "def get_morph_net(target_global_feature,source_global_feature,source_points_features,is_training_pl):\n",
    "    num_point = source_points_features.get_shape()[1].value\n",
    "    target_global_feature = tf.expand_dims(tf.expand_dims(target_global_feature,dim =1),dim=2)\n",
    "    source_global_feature = tf.expand_dims(tf.expand_dims(source_global_feature,dim =1),dim=2)\n",
    "\n",
    "\n",
    "    print(target_global_feature)\n",
    "    target_global_feature_expand = tf.tile(target_global_feature,[1, num_point ,1,1])\n",
    "    target_global_feature_expand = tf.tile(source_global_feature,[1, num_point ,1,1])\n",
    "    concat_feat = tf.concat([source_points_features,\n",
    "                               target_global_feature_expand,target_global_feature_expand],axis=3)\n",
    "    net = tf_util.conv2d(concat_feat, 512, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 256, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training_pl,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 3, [1,1],\n",
    "                         padding='VALID', stride=[1,1], activation_fn=None,\n",
    "                         scope='conv10')\n",
    "\n",
    "    net = tf.squeeze(net, [2]) # BxNx3\n",
    "    return net\n",
    "\n",
    "def get_loss_func(target_global_feature,source_to_target_feature,source_to_traget_points,source_points):\n",
    "        \n",
    "    margin = 0.2\n",
    "    d_eucd2 = tf.pow(tf.subtract(source_points,source_to_traget_points),2)\n",
    "    d_eucd2 = tf.reduce_sum(d_eucd2, 1)\n",
    "    d_eucd = tf.sqrt(d_eucd2+1e-6, name=\"d_eucd\")\n",
    "    C = tf.constant(margin, name=\"C\")\n",
    "    d_eucd = tf.maximum(tf.subtract(d_eucd,C),0)\n",
    "        \n",
    "    f_eucd2 = tf.pow(tf.subtract(target_global_feature,source_to_target_feature),2)\n",
    "    f_eucd2 = tf.reduce_sum(f_eucd2,1)\n",
    "    f_eucd = tf.sqrt(f_eucd2+1e-6, name=\"f_eucd\")\n",
    "    losses = tf.add(d_eucd, f_eucd, name=\"losses\")\n",
    "    loss = tf.reduce_mean(losses, name=\"loss\")\n",
    "    return loss\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(sess,ops,train_writer):\n",
    "    is_traning_pl = True\n",
    "    train_file_idxs = np.arrange(0,len(Train_FILES))\n",
    "    np.random.shuffle(train_file_idxs)\n",
    "    \n",
    "    for fn in range(len(Train_FILES)):\n",
    "        log_string('----'+str(fn)+'----')\n",
    "        current_points1, current_points2 = provider_zj.load_h5(Train_FILES[train_file_idxs[fn]])\n",
    "        current_points1 = current_points1[:,0:NUM_POINT,:]\n",
    "        current_points2 = current_points2[:,0:NUM_POINT,:]\n",
    "        file_size = current_points1.shape[0]\n",
    "        num_batches = file_size/BATCH_SIZE\n",
    "        loss_sum = 0\n",
    "        \n",
    "        #for batch_idx in range(num_batches-1):\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"features/maxpooling:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"features/maxpooling_1:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n",
      "(32, 1024, 1, 64)\n",
      "Tensor(\"morph/ExpandDims_1:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"features_1/maxpooling:0\", shape=(32, 1, 1, 1024), dtype=float32, device=/device:GPU:0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eucd2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3267ab7ef21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             net_new, points_feature_new, end_points_new = extract_global_feature_net(new_points,\n\u001b[1;32m     27\u001b[0m                                                                                      is_training_pl,bn_decay)\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints_feature1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoints_feature_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_points\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoints2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4bf5e52b08c4>\u001b[0m in \u001b[0;36mget_loss_func\u001b[0;34m(target_global_feature, source_to_target_feature, source_to_traget_points, source_points)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mf_eucd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_global_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_to_target_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mf_eucd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meucd2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mf_eucd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meucd2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f_eucd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_eucd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_eucd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eucd2' is not defined"
     ]
    }
   ],
   "source": [
    "# train files load\n",
    "TRAIN_FILES = provider_zj.getDataFiles('/Users/jinwei/Dataset/corres_1024/train_files.txt')\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "        points1 = tf.placeholder(tf.float32, shape=(BATCH_SIZE,NUM_POINT,3))\n",
    "        points2 = tf.placeholder(tf.float32, shape=(BATCH_SIZE,NUM_POINT,3))\n",
    "        batch = tf.Variable(0)\n",
    "        bn_decay = get_bn_decay(batch)\n",
    "        tf.summary.scalar('bn_decay',bn_decay)\n",
    "        with tf.variable_scope('features') as sc:\n",
    "            # target\n",
    "            net1, points_feature1, end_points1 = extract_global_feature_net(points1,is_training_pl,bn_decay)\n",
    "                                                                        \n",
    "            sc.reuse_variables()\n",
    "            # source\n",
    "            net2, points_feature2, end_points2 = extract_global_feature_net(points2,is_training_pl,bn_decay)\n",
    "            \n",
    "                                                                            \n",
    "        print(points_feature2.get_shape())                                                                \n",
    "                                                                            \n",
    "        with tf.variable_scope('morph') as sc:\n",
    "            new_points = get_morph_net(net1,net2,points_feature2,is_training_pl)\n",
    "        with tf.variable_scope('features') as sc:\n",
    "            sc.reuse_variables()\n",
    "            net_new, points_feature_new, end_points_new = extract_global_feature_net(new_points,\n",
    "                                                                                     is_training_pl,bn_decay)\n",
    "        loss = get_loss_func(points_feature1,points_feature_new,new_points,points2)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"features/Reshape:0\", shape=(32, 1024), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(32, 1, 1, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(net1)\n",
    "print(tf.expand_dims(tf.expand_dims(net1,dim =1),dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 'sdsd'\n",
    "b = 'ddd'+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
